{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32abe7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d305c",
   "metadata": {},
   "source": [
    "# Pickup all excel sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977cf402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/windowsC/SIN_Desktop/ENCHANT/wp3_datasets/briskee/briskee_data_final.csv']\n",
      "1\n",
      "/home/kalyan/gitRepos/sin-Enchant/utils/../../briskee/plots directory: exists\n"
     ]
    }
   ],
   "source": [
    "SURVEY_LIST = ['config_echoes.yaml', 'config_briskee.yaml', 'config_cheetah.yaml', 'config_natc.yaml']\n",
    "SURVEY_CHOICE = 1\n",
    "\n",
    "with open(SURVEY_LIST[SURVEY_CHOICE], 'r') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "data_files = []\n",
    "dirs = list(set([__dir if os.path.isdir(os.path.join(args['data_dir'], __dir)) else '' for __dir in os.listdir(args['data_dir'])]))\n",
    "for _dir in dirs:\n",
    "    for filename in os.listdir(os.path.join(args['data_dir'], _dir)):\n",
    "        if args['format'] == 'rda': # scan for .rdata/.rda\n",
    "            if filename.endswith(\".rda\") or filename.endswith(\".rdata\"): \n",
    "                data_files.append(os.path.join(args['data_dir'], _dir, filename))\n",
    "            else:\n",
    "                continue\n",
    "        elif args['format'] == 'xls': # scan for .xls/.xlsx\n",
    "            if filename.endswith(\".xls\") or filename.endswith(\".xlsx\"): \n",
    "                 data_files.append(os.path.join(args['data_dir'], _dir, filename))\n",
    "            else:\n",
    "                continue\n",
    "        elif args['format'] == 'csv': # scan for .csv\n",
    "            if filename.endswith(\".csv\"): \n",
    "                 data_files.append(os.path.join(args['data_dir'], _dir, filename))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print('Unsupported file format selcted')\n",
    "\n",
    "print(data_files)\n",
    "print(len(data_files))\n",
    "\n",
    "out_dir = os.path.join(os.getcwd(), args['out_dir'])\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "else:\n",
    "    print(f\"{out_dir} directory: exists\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2536e",
   "metadata": {},
   "source": [
    "# Sanity checks and iterate over individual sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7311f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get_csv_df_list', 'get_spss_df_list', 'get_xls_df_list']\n",
      "csv: <function SurveyParser.get_csv_df_list at 0x7f98408b7440>\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import inspect\n",
    "\n",
    "class SurveyParser(object):\n",
    "    \n",
    "    default_args = {\n",
    "        'separator': ',',\n",
    "        'ignore_files_with_phrases': ['codebook', 'codes_labels']\n",
    "    }\n",
    "    \n",
    "    def get_xls_df_list(self, \n",
    "                        _data_files, \n",
    "                        **kwargs):\n",
    "        survey_df_dict = {}\n",
    "        remove_files_with_phrases = kwargs.default_args['ignore_files_with_phrases'] if 'ignore_files_with_phrases' in kwargs.keys() else self.default_args['ignore_files_with_phrases']\n",
    "        data_files = [_file for _file in _data_files if not any(phrase in _file for phrase in remove_files_with_phrases)]\n",
    "\n",
    "        for _file in data_files:\n",
    "            try:\n",
    "                sheet = pd.read_excel(_file, sheet_name=0) # Access the first sheet the other are codes and questionaire interpretations\n",
    "            except Exception as e:\n",
    "                print('Exception caught in xls')\n",
    "\n",
    "                _csv = '.'.join(_file.split('.')[:-1])+'.csv'\n",
    "                if os.path.exists(_csv+'.0'):\n",
    "                    print('File exists: Skip xls to csv conversion')\n",
    "                else:\n",
    "                    #pdb.set_trace()\n",
    "                    # Try converting xls to csv through CLI\n",
    "                    cmd = ['ssconvert', '-S', _file, _csv]\n",
    "                    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "                    o, e = proc.communicate()\n",
    "\n",
    "                sheet = pd.read_csv(_csv+'.0')\n",
    "\n",
    "            name = _file.replace(\" \", \"_\").split('/')[-1]\n",
    "            survey_df_dict[name] = sheet\n",
    "            sheet = None\n",
    "\n",
    "        return survey_df_dict\n",
    "\n",
    "    def get_csv_df_list(self, \n",
    "                        _data_files, \n",
    "                        **kwargs):\n",
    "        survey_df_dict = {}\n",
    "        remove_files_with_phrases = kwargs['ignore_files_with_phrases'] if 'ignore_files_with_phrases' in kwargs.keys() else self.default_args['ignore_files_with_phrases']\n",
    "        separator = kwargs['separator'] if 'separator' in kwargs.keys() else self.default_args['separator']\n",
    "        data_files = [_file for _file in _data_files if not any(phrase in _file for phrase in remove_files_with_phrases)]\n",
    "        \n",
    "        for _file in data_files:\n",
    "            sheet = pd.read_csv(_file, \n",
    "                                sep=separator,\n",
    "                                encoding='unicode_escape',\n",
    "                               low_memory=False)\n",
    "\n",
    "            name = _file.replace(\" \", \"_\").split('/')[-1]\n",
    "            survey_df_dict[name] = sheet\n",
    "            sheet = None\n",
    "\n",
    "        return survey_df_dict\n",
    "\n",
    "    def get_spss_df_list(self, \n",
    "                        _data_files, \n",
    "                        **kwargs):\n",
    "        return survey_df_dict\n",
    "\n",
    "    \n",
    "def get_methods(class_arg):\n",
    "\n",
    "    assert inspect.isclass(class_arg), 'Expecting a class as an argument'\n",
    "    \n",
    "    method_list = []\n",
    "    # attribute is a string representing the attribute name\n",
    "    for attribute in dir(class_arg):\n",
    "        # Get the attribute value\n",
    "        attribute_value = getattr(class_arg, attribute)\n",
    "        # Check that it is callable\n",
    "        if callable(attribute_value):\n",
    "            # Filter all dunder (__ prefix) methods\n",
    "            if attribute.startswith('__') == False:\n",
    "                method_list.append(attribute)\n",
    "\n",
    "    return method_list\n",
    "\n",
    "\n",
    "def get_method_fn(method,\n",
    "                  method_key,\n",
    "                  methods_list,\n",
    "                  methods_class):\n",
    "    \n",
    "    assert method in methods_list, f'Error: Improper method selected in {method_key} in config.yaml'\n",
    "    fn_method = getattr(methods_class, method)\n",
    "    print(f\"{method_key}: {fn_method}\")\n",
    "    return fn_method\n",
    "\n",
    "\n",
    "parser_class_inst = SurveyParser()\n",
    "parser_list = get_methods(SurveyParser)\n",
    "print(parser_list)\n",
    "parser_fn = get_method_fn(f\"get_{args['format']}_df_list\",\n",
    "                          args['format'],\n",
    "                          parser_list,\n",
    "                          SurveyParser)\n",
    "\n",
    "survey_df_dict = parser_fn(parser_class_inst, data_files, **args)  # Example of procedure call with class instance passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0012276f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "briskee_data_final.csv: (15055, 258)\n"
     ]
    }
   ],
   "source": [
    "'''TODO: Handle None dfs'''\n",
    "for name, survey_df in survey_df_dict.items():\n",
    "    print(f\"{name}: {survey_df.shape}\")\n",
    "#print(survey_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474fb312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurveyConditioning(object):\n",
    "    \n",
    "    def get_nan_idices(sheet):\n",
    "        '''\n",
    "        '''\n",
    "        idx = np.argwhere(pd.isnull(sheet).to_numpy()) #2D indices\n",
    "        #print(idx)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "    def no_nan(_sheet):\n",
    "        '''\n",
    "        '''\n",
    "        sheet = _sheet.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "        #print(sheet.shape)\n",
    "        return sheet\n",
    "\n",
    "    def substitute_nan(_sheet):\n",
    "        '''\n",
    "        '''\n",
    "        sheet = _sheet.fillna(-1.0)\n",
    "        #print(sheet.shape)\n",
    "        return sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c42d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get_nan_idices', 'no_nan', 'substitute_nan']\n"
     ]
    }
   ],
   "source": [
    "conditioning_class_inst = SurveyConditioning()\n",
    "methods_list = get_methods(SurveyConditioning)\n",
    "print(methods_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677854ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format': 'csv', 'separator': ';', 'data_dir': '/media/windowsC/SIN_Desktop/ENCHANT/wp3_datasets/briskee', 'out_dir': '../../briskee/plots', 'surveys': {'global': {'histogram': {'enable': True, 'ROWS': 26, 'COLS': 10, 'SIZE': ['8*8', '6*8'], 'padding': 1.0}, 'conditioning': 'substitute_nan'}, 'briskee_data_final': {'histogram': {'enable': True}}}}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53b502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['briskee_data_final'])\n",
      "\u001b[33m============================================ briskee_data_final ============================================\u001b[0m\n",
      "survey_global_settings:{'histogram': {'enable': True, 'ROWS': 26, 'COLS': 10, 'SIZE': ['8*8', '6*8'], 'padding': 1.0}, 'conditioning': 'substitute_nan'}\n",
      ", val:{'histogram': {'enable': True, 'ROWS': 26, 'COLS': 10, 'SIZE': ['8*8', '6*8'], 'padding': 1.0}, 'conditioning': 'substitute_nan'}\n",
      "briskee_data_final: <function SurveyConditioning.substitute_nan at 0x7f9840874290>\n",
      "before:(15055, 258), after:(15055, 258)\n",
      "\u001b[32m================= Heap: 865.0 MB =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PlottingClass:\n",
    "    MAX_LABEL_LEN = 5\n",
    "    \n",
    "    def labels_consistency(self,\n",
    "                           label):\n",
    "        return f\"{str(label)[:self.MAX_LABEL_LEN]}...\"\n",
    "            \n",
    "    def build_histogram(self,\n",
    "                        out_dir,\n",
    "                        parsed_df,\n",
    "                        settings,\n",
    "                        survey_name,\n",
    "                        ignore_label=-1.0):\n",
    "        #df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'], 'population': [1864, 22000, 80000]}, index=['panda', 'polar', 'koala'])\n",
    "        parsed_df = parsed_df.iloc[settings.get(\"n_discard_columns\", 1):, :]\n",
    "        ignore_labels = settings.get(\"ignore_labels\", [ignore_label])\n",
    "\n",
    "        ROWS, COLS, SIZE = settings.get(\"ROWS\", 0), settings.get(\"COLS\", 0), settings.get(\"SIZE\", (['8*8', '6*8']))\n",
    "        assert not (ROWS == 0 and COLS == 0), f'{survey_name} Error: ROWS/COL value not specified in config.yaml'\n",
    "\n",
    "        SIZE = list(map(lambda x: eval(x), SIZE))\n",
    "        fig, axes = plt.subplots(figsize=SIZE, dpi=300, nrows=ROWS, ncols=COLS)\n",
    "\n",
    "        idx = 0\n",
    "        for name, col in parsed_df.items():\n",
    "            #assert all(col.map(type) == int) or all(col.map(type) == float)\n",
    "            sr_count = col.value_counts()\n",
    "            _ignore_labels = [label for label in ignore_labels if label in sr_count.keys()]\n",
    "            if len(_ignore_labels) > 0:\n",
    "                try:\n",
    "                    sr_count.drop(labels=_ignore_labels, inplace=True)\n",
    "                except KeyError:\n",
    "                    pdb.set_trace()\n",
    "\n",
    "            i, j = idx // COLS, idx % COLS\n",
    "            if i>= ROWS or j>= COLS:\n",
    "                pdb.set_trace()\n",
    "            #print(sr_count.keys().dtype, sr_count.values.dtype)\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if sr_count.sum() > 0:  #Plot the responses in corresponding subplot\n",
    "                if not (all(isinstance(x, (int,float)) for x in sr_count.keys())):\n",
    "                    sr_count.index = list(map(self.labels_consistency, list(sr_count.keys())))\n",
    "                \n",
    "                try:\n",
    "                    ax.bar(sr_count.keys(), sr_count.values, align='center', color='#607c8e')\n",
    "                    ax.plot(sr_count.keys(), sr_count.values, marker=\"o\", linestyle=\"\", alpha=0.8, color=\"b\")\n",
    "                    FIRST_TIME = False\n",
    "                except TypeError:\n",
    "                    pdb.set_trace()\n",
    "                \n",
    "                ax.set_title(f'{name}', fontsize=7, fontweight='bold', color='blue')\n",
    "\n",
    "                ax.xaxis.set_tick_params(labelsize=5, labelcolor='r')  #, labelrotation=-60)\n",
    "                ax.set_xlabel(f'response[{sr_count.sum()}]', fontsize=5, fontweight='bold')\n",
    "                ax.yaxis.set_tick_params(labelsize=5, labelcolor='r')\n",
    "                ax.set_ylabel('count', fontsize=5, fontweight='bold')\n",
    "                ax.grid(axis='y', alpha=0.75)\n",
    "                #plt.setp(ax.set_xticklabels(sr_count.keys()))\n",
    "                idx += 1\n",
    "\n",
    "        if idx < ROWS*COLS:  #Remove excess subplots\n",
    "            remove_axes = list(range(idx, ROWS*COLS))\n",
    "            for idx in remove_axes:\n",
    "                i, j = idx // COLS, idx % COLS\n",
    "                ax = axes[i, j]\n",
    "                ax.remove()\n",
    "\n",
    "        padding = settings.get(\"padding\", 3.0)\n",
    "        fig.tight_layout(pad=padding)\n",
    "        plt.savefig(os.path.join(out_dir,f'{survey_name}.png'), bbox_inches='tight')\n",
    "\n",
    "        fig.clf()\n",
    "        plt.close(fig)\n",
    "        plt.close('all')\n",
    "\n",
    "try:\n",
    "    survey_global_settings = args['surveys'].pop('global')\n",
    "except \"KeyError\":\n",
    "    survey_global_settings = None\n",
    "\n",
    "def recursive_parse_settings(given_setting, modify_setting):\n",
    "    for key, val in modify_setting.items():\n",
    "        if isinstance(val, dict):\n",
    "            recursive_parse_settings(given_setting[key], val)\n",
    "        else:\n",
    "            given_setting[key] = val\n",
    "    return given_setting\n",
    "\n",
    "\n",
    "from termcolor import colored\n",
    "import itertools\n",
    "from guppy import hpy\n",
    "import warnings\n",
    "warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )\n",
    "\n",
    "\n",
    "print(args['surveys'].keys())\n",
    "START_FROM = 0\n",
    "surveys = dict(itertools.islice(args['surveys'].items(), START_FROM, len(args['surveys'])))\n",
    "\n",
    "heap = hpy()\n",
    "heap.setref()\n",
    "\n",
    "memory_profile = []\n",
    "heap_start = heap.heap()\n",
    "\n",
    "for key, _val in surveys.items():\n",
    "    #pdb.set_trace()\n",
    "    global_settings = deepcopy(survey_global_settings)\n",
    "    print(colored(f'============================================ {key} ============================================', 'yellow'))\n",
    "    val = recursive_parse_settings(global_settings, _val) if survey_global_settings is not None else _val\n",
    "    print(f\"survey_global_settings:{survey_global_settings}\\n, val:{val}\")\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    method_fn = get_method_fn(val['conditioning'],\n",
    "                              key,\n",
    "                              methods_list,\n",
    "                              SurveyConditioning)\n",
    "\n",
    "    matching_df = [survey_df for file_name, survey_df in survey_df_dict.items() if key in file_name]\n",
    "    assert len(matching_df) > 0, 'Error: Improper survey name in config.yaml'\n",
    "    parsed_df = method_fn(matching_df[0])  # Example of procedure call without class instance passing\n",
    "    print(f\"before:{matching_df[0].shape}, after:{parsed_df.shape}\")\n",
    "\n",
    "    pltClass = PlottingClass()\n",
    "    pltClass.MAX_LABEL_LEN = global_settings.get(\"MAX_LABEL_LEN\", 5)\n",
    "    \n",
    "    # Build a histogram of each column in df\n",
    "    if val['histogram']['enable']:\n",
    "        pltClass.build_histogram(out_dir, \n",
    "                    parsed_df,\n",
    "                    val['histogram'], \n",
    "                    f\"{key}_{val['conditioning']}\")\n",
    "    del pltClass, global_settings, val, method_fn, matching_df, parsed_df\n",
    "    gc.collect\n",
    "\n",
    "    heap_end = heap.heap()\n",
    "    curr_heap = (heap_end.size-heap_start.size)//1e3\n",
    "    memory_profile.append(curr_heap)\n",
    "    print(colored(f'================= Heap: {curr_heap} MB =================', 'green'))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(memory_profile)\n",
    "plt.savefig(os.path.join('./','memory_profile.png'), bbox_inches='tight')\n",
    "\n",
    "fig.clf()\n",
    "plt.close(fig)\n",
    "plt.close('all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318c462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
